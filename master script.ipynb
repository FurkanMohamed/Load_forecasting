{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f49f267c-37d9-4fd7-ba28-32f682ac7533",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best station for each load zone based on maximum correlation:\n",
      "         zone_id  station_id  correlation\n",
      "zone_id                                  \n",
      "1            1.0         8.0    -0.072835\n",
      "2            2.0        10.0    -0.153490\n",
      "3            3.0         1.0    -0.119071\n",
      "4            4.0         8.0    -0.206846\n",
      "5            5.0         8.0     0.011229\n",
      "6            6.0         1.0    -0.119579\n",
      "7            7.0         1.0    -0.126853\n",
      "8            8.0         6.0    -0.013531\n",
      "9            9.0         1.0    -0.186813\n",
      "10          10.0         8.0    -0.215592\n",
      "11          11.0         1.0    -0.424392\n",
      "12          12.0        10.0    -0.031053\n",
      "13          13.0         8.0    -0.129365\n",
      "14          14.0         1.0    -0.302143\n",
      "15          15.0         3.0    -0.091166\n",
      "16          16.0        10.0    -0.095418\n",
      "17          17.0         1.0    -0.119998\n",
      "18          18.0         1.0    -0.385365\n",
      "19          19.0         2.0    -0.061400\n",
      "20          20.0         8.0    -0.205292\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "load_data = pd.read_csv('../Load_history_final.csv')\n",
    "temp_data = pd.read_csv('../Temp_history_final.csv')\n",
    "\n",
    "# Function to create a datetime column from year, month, and day\n",
    "def create_datetime(df):\n",
    "    # Ensure year, month, and day are integers and create a datetime column\n",
    "    df['datetime'] = pd.to_datetime(df[['year', 'month', 'day']].astype(int))\n",
    "    return df\n",
    "\n",
    "# Apply the function to both datasets\n",
    "load_data = create_datetime(load_data)\n",
    "temp_data = create_datetime(temp_data)\n",
    "\n",
    "# Exclude data from June 2008\n",
    "load_data = load_data[~((load_data['datetime'].dt.month == 6) & (load_data['datetime'].dt.year == 2008))]\n",
    "temp_data = temp_data[~((temp_data['datetime'].dt.month == 6) & (temp_data['datetime'].dt.year == 2008))]\n",
    "\n",
    "# Melting the data to convert hours columns into rows\n",
    "def melt_data(df, id_vars, value_name):\n",
    "    return df.melt(id_vars=id_vars, var_name='hour', value_vars=[f'h{i}' for i in range(1, 25)], value_name=value_name)\n",
    "\n",
    "load_data_melt = melt_data(load_data, ['zone_id', 'datetime'], 'load')\n",
    "temp_data_melt = melt_data(temp_data, ['station_id', 'datetime'], 'temperature')\n",
    "\n",
    "# Merge the datasets on datetime and hour\n",
    "combined_data = pd.merge(load_data_melt, temp_data_melt, on=['datetime', 'hour'])\n",
    "\n",
    "# Calculate correlations and determine the best station for each zone\n",
    "zone_station_correlation = combined_data.groupby(['zone_id', 'station_id']).apply(lambda df: df['load'].corr(df['temperature']))\n",
    "\n",
    "# Reset index so we can access the correlation values\n",
    "correlation_data = zone_station_correlation.reset_index(name='correlation')\n",
    "\n",
    "# Handle NaN correlations that can occur if there is no variance in temperature or load\n",
    "correlation_data = correlation_data.dropna(subset=['correlation'])\n",
    "\n",
    "# Determine the best station for each zone based on the highest correlation\n",
    "def get_best_station(group):\n",
    "    return group.loc[group['correlation'].idxmax()]\n",
    "\n",
    "best_stations = correlation_data.groupby('zone_id').apply(get_best_station)\n",
    "\n",
    "# Output results\n",
    "print(\"Best station for each load zone based on maximum correlation:\")\n",
    "print(best_stations[['zone_id', 'station_id', 'correlation']])\n",
    "\n",
    "# Optionally, save the merged dataset for further analysis\n",
    "combined_data.to_csv('combined_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d2b1a12-7afb-480c-a786-eff3343772dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zone 1: Best Parameters = {'max_depth': 10, 'min_samples_split': 6, 'n_estimators': 150}\n",
      "Zone 1: Training Score = 0.7523, Validation Score = 0.7278, Test Score = 0.7280\n",
      "Zone 2: Best Parameters = {'max_depth': 10, 'min_samples_split': 6, 'n_estimators': 150}\n",
      "Zone 2: Training Score = 0.7754, Validation Score = 0.7562, Test Score = 0.7582\n",
      "Zone 3: Best Parameters = {'max_depth': 10, 'min_samples_split': 6, 'n_estimators': 150}\n",
      "Zone 3: Training Score = 0.7207, Validation Score = 0.6853, Test Score = 0.6966\n",
      "Zone 4: Best Parameters = {'max_depth': 10, 'min_samples_split': 6, 'n_estimators': 150}\n",
      "Zone 4: Training Score = 0.8160, Validation Score = 0.7911, Test Score = 0.7964\n",
      "Zone 5: Best Parameters = {'max_depth': 10, 'min_samples_split': 6, 'n_estimators': 150}\n",
      "Zone 5: Training Score = 0.7587, Validation Score = 0.7334, Test Score = 0.7341\n",
      "Zone 6: Best Parameters = {'max_depth': 10, 'min_samples_split': 6, 'n_estimators': 50}\n",
      "Zone 6: Training Score = 0.7210, Validation Score = 0.6855, Test Score = 0.6969\n",
      "Zone 7: Best Parameters = {'max_depth': 10, 'min_samples_split': 6, 'n_estimators': 50}\n",
      "Zone 7: Training Score = 0.7338, Validation Score = 0.6990, Test Score = 0.7102\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Initialize Grid Search with cross-validation\u001b[39;00m\n\u001b[0;32m     48\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(RandomForestRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m), param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, scoring\u001b[38;5;241m=\u001b[39mmake_scorer(r2_score))\n\u001b[1;32m---> 49\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     51\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Evaluate the best model on the training, validation, and test sets\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    462\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    465\u001b[0m ]\n\u001b[0;32m    467\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    474\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    475\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    476\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    477\u001b[0m )(\n\u001b[0;32m    478\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    479\u001b[0m         t,\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[0;32m    481\u001b[0m         X,\n\u001b[0;32m    482\u001b[0m         y,\n\u001b[0;32m    483\u001b[0m         sample_weight,\n\u001b[0;32m    484\u001b[0m         i,\n\u001b[0;32m    485\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[0;32m    486\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    487\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m    488\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[0;32m    489\u001b[0m     )\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    491\u001b[0m )\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:184\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    182\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 184\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1247\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m \n\u001b[0;32m   1221\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1247\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m   1248\u001b[0m         X,\n\u001b[0;32m   1249\u001b[0m         y,\n\u001b[0;32m   1250\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1251\u001b[0m         check_input\u001b[38;5;241m=\u001b[39mcheck_input,\n\u001b[0;32m   1252\u001b[0m     )\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "combined_data = pd.read_csv('combined_data.csv')\n",
    "combined_data['datetime'] = pd.to_datetime(combined_data['datetime'])\n",
    "combined_data['hour'] = pd.to_numeric(combined_data['hour'].str.extract('(\\d+)')[0])\n",
    "\n",
    "# Zone to station mapping based on best correlation\n",
    "zone_station_map = {\n",
    "    1: 8, 2: 10, 3: 1, 4: 8, 5: 8, 6: 1, 7: 1, 8: 6, \n",
    "    9: 1, 10: 8, 11: 1, 12: 10, 13: 8, 14: 1, 15: 3, \n",
    "    16: 10, 17: 1, 18: 1, 19: 2, 20: 8\n",
    "}\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 4, 6]\n",
    "}\n",
    "\n",
    "# Store models and their scores for each zone\n",
    "models = {}\n",
    "model_scores = {}\n",
    "\n",
    "for zone, station in zone_station_map.items():\n",
    "    # Filter data for this zone and station\n",
    "    zone_data = combined_data[(combined_data['zone_id'] == zone) & (combined_data['station_id'] == station)]\n",
    "    \n",
    "    # Apply specific filter for zone 15\n",
    "    if zone == 15:\n",
    "        zone_data = zone_data[zone_data['load'] < 200000]  # Exclude load values above 200,000 for zone 15\n",
    "    \n",
    "    # Features and target\n",
    "    X = zone_data[['temperature', 'hour']]\n",
    "    y = zone_data['load']\n",
    "    \n",
    "    # Split data into training/validation (80%) and test (20%) sets\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Further split the training/validation data into training (80%) and validation (20%) sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "    \n",
    "    # Initialize Grid Search with cross-validation\n",
    "    grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=3, scoring=make_scorer(r2_score))\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Evaluate the best model on the training, validation, and test sets\n",
    "    train_score = best_model.score(X_train, y_train)\n",
    "    val_score = best_model.score(X_val, y_val)\n",
    "    test_score = best_model.score(X_test, y_test)\n",
    "\n",
    "    # Store the best model and scores\n",
    "    models[zone] = best_model\n",
    "    model_scores[zone] = {'Training Score': train_score, 'Validation Score': val_score, 'Test Score': test_score}\n",
    "    \n",
    "    # Print model performance and best parameters for each zone\n",
    "    print(f'Zone {zone}: Best Parameters = {grid_search.best_params_}')\n",
    "    print(f'Zone {zone}: Training Score = {train_score:.4f}, Validation Score = {val_score:.4f}, Test Score = {test_score:.4f}')\n",
    "\n",
    "# Calculate weighted average scores\n",
    "general_train_score = total_train_score / total_samples_train\n",
    "general_val_score = total_val_score / total_samples_val\n",
    "general_test_score = total_test_score / total_samples_test\n",
    "\n",
    "# Output the general scores\n",
    "print(f'General Training Score: {general_train_score:.4f}')\n",
    "print(f'General Validation Score: {general_val_score:.4f}')\n",
    "print(f'General Test Score: {general_test_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "889d4861-a792-42c6-b4bb-0020e0b3a3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Training Score: 0.7248\n",
      "General Validation Score: 0.6969\n",
      "General Test Score: 0.7024\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load the dataset\n",
    "combined_data = pd.read_csv('combined_data.csv')\n",
    "combined_data['datetime'] = pd.to_datetime(combined_data['datetime'])\n",
    "combined_data['hour'] = pd.to_numeric(combined_data['hour'].str.extract('(\\d+)')[0])\n",
    "\n",
    "# Zone to station mapping based on best correlation\n",
    "zone_station_map = {\n",
    "    1: 8, 2: 10, 3: 1, 4: 8, 5: 8, 6: 1, 7: 1, 8: 6, \n",
    "    9: 1, 10: 8, 11: 1, 12: 10, 13: 8, 14: 1, 15: 3, \n",
    "    16: 10, 17: 1, 18: 1, 19: 2, 20: 8\n",
    "}\n",
    "\n",
    "# Initialize accumulators for weighted average calculations\n",
    "total_train_score = 0\n",
    "total_val_score = 0\n",
    "total_test_score = 0\n",
    "total_samples_train = 0\n",
    "total_samples_val = 0\n",
    "total_samples_test = 0\n",
    "\n",
    "for zone, station in zone_station_map.items():\n",
    "    # Filter data for this zone and station\n",
    "    zone_data = combined_data[(combined_data['zone_id'] == zone) & (combined_data['station_id'] == station)]\n",
    "\n",
    "    # Apply specific filter for zone 15\n",
    "    if zone == 15:\n",
    "        zone_data = zone_data[zone_data['load'] < 200000]  # Exclude load values above 200,000 for zone 15\n",
    "    \n",
    "    # Features and target\n",
    "    X = zone_data[['temperature', 'hour']]\n",
    "    y = zone_data['load']\n",
    "    \n",
    "    # Split data into training/validation (80%) and test (20%) sets\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Further split the training/validation data into training (80%) and validation (20%) sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)  # Note: 0.25 * 0.8 = 0.2\n",
    "    \n",
    "    # Initialize and train the Random Forest Regressor\n",
    "    model = RandomForestRegressor(n_estimators=150, max_depth = 10, min_samples_split = 6, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate scores\n",
    "    train_score = model.score(X_train, y_train)\n",
    "    val_score = model.score(X_val, y_val)\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    \n",
    "    # Accumulate scores weighted by number of samples\n",
    "    total_train_score += train_score * len(y_train)\n",
    "    total_val_score += val_score * len(y_val)\n",
    "    total_test_score += test_score * len(y_test)\n",
    "    total_samples_train += len(y_train)\n",
    "    total_samples_val += len(y_val)\n",
    "    total_samples_test += len(y_test)\n",
    "\n",
    "# Calculate weighted average scores\n",
    "general_train_score = total_train_score / total_samples_train\n",
    "general_val_score = total_val_score / total_samples_val\n",
    "general_test_score = total_test_score / total_samples_test\n",
    "\n",
    "# Output the general scores\n",
    "print(f'General Training Score: {general_train_score:.4f}')\n",
    "print(f'General Validation Score: {general_val_score:.4f}')\n",
    "print(f'General Test Score: {general_test_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3693d3ab-92e0-4b53-bd92-e3cb761a8590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zone 1: Best Parameters = {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 150}\n",
      "Zone 1: Training Score = 0.7461, Validation Score = 0.7359, Test Score = 0.7351\n",
      "Zone 2: Best Parameters = {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 150}\n",
      "Zone 2: Training Score = 0.7691, Validation Score = 0.7616, Test Score = 0.7650\n",
      "Zone 3: Best Parameters = {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 150}\n",
      "Zone 3: Training Score = 0.7160, Validation Score = 0.6948, Test Score = 0.7030\n",
      "Zone 4: Best Parameters = {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 150}\n",
      "Zone 4: Training Score = 0.8111, Validation Score = 0.8003, Test Score = 0.8035\n",
      "Zone 5: Best Parameters = {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 150}\n",
      "Zone 5: Training Score = 0.7515, Validation Score = 0.7425, Test Score = 0.7411\n",
      "Zone 6: Best Parameters = {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 150}\n",
      "Zone 6: Training Score = 0.7166, Validation Score = 0.6957, Test Score = 0.7033\n",
      "Zone 7: Best Parameters = {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 150}\n",
      "Zone 7: Training Score = 0.7290, Validation Score = 0.7084, Test Score = 0.7164\n",
      "Zone 8: Best Parameters = {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 150}\n",
      "Zone 8: Training Score = 0.7777, Validation Score = 0.7617, Test Score = 0.7682\n",
      "Zone 9: Best Parameters = {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}\n",
      "Zone 9: Training Score = 0.7401, Validation Score = 0.7179, Test Score = 0.7281\n",
      "Zone 10: Best Parameters = {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 150}\n",
      "Zone 10: Training Score = 0.7761, Validation Score = 0.7716, Test Score = 0.7723\n",
      "Zone 11: Best Parameters = {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 150}\n",
      "Zone 11: Training Score = 0.7399, Validation Score = 0.7475, Test Score = 0.7470\n",
      "Zone 12: Best Parameters = {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 150}\n",
      "Zone 12: Training Score = 0.7266, Validation Score = 0.7225, Test Score = 0.7246\n",
      "Zone 13: Best Parameters = {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 150}\n",
      "Zone 13: Training Score = 0.7148, Validation Score = 0.7080, Test Score = 0.7139\n",
      "Zone 14: Best Parameters = {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 150}\n",
      "Zone 14: Training Score = 0.7505, Validation Score = 0.7400, Test Score = 0.7420\n",
      "Zone 15: Best Parameters = {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 150}\n",
      "Zone 15: Training Score = 0.6505, Validation Score = 0.6234, Test Score = 0.6383\n",
      "Zone 16: Best Parameters = {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 150}\n",
      "Zone 16: Training Score = 0.7557, Validation Score = 0.7442, Test Score = 0.7490\n",
      "Zone 17: Best Parameters = {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 150}\n",
      "Zone 17: Training Score = 0.7165, Validation Score = 0.6945, Test Score = 0.7040\n",
      "Zone 18: Best Parameters = {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 150}\n",
      "Zone 18: Training Score = 0.7530, Validation Score = 0.7468, Test Score = 0.7528\n",
      "Zone 19: Best Parameters = {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 100}\n",
      "Zone 19: Training Score = 0.2015, Validation Score = 0.1674, Test Score = 0.1773\n",
      "Zone 20: Best Parameters = {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 150}\n",
      "Zone 20: Training Score = 0.8059, Validation Score = 0.8008, Test Score = 0.8002\n",
      "General Training Score: 0.7177\n",
      "General Validation Score: 0.7047\n",
      "General Test Score: 0.7096\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load the dataset\n",
    "combined_data = pd.read_csv('combined_data.csv')\n",
    "combined_data['datetime'] = pd.to_datetime(combined_data['datetime'])\n",
    "combined_data['hour'] = pd.to_numeric(combined_data['hour'].str.extract('(\\d+)')[0])\n",
    "\n",
    "# Zone to station mapping based on best correlation\n",
    "zone_station_map = {\n",
    "    1: 8, 2: 10, 3: 1, 4: 8, 5: 8, 6: 1, 7: 1, 8: 6, \n",
    "    9: 1, 10: 8, 11: 1, 12: 10, 13: 8, 14: 1, 15: 3, \n",
    "    16: 10, 17: 1, 18: 1, 19: 2, 20: 8\n",
    "}\n",
    "\n",
    "# Define the parameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 5, 8],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Initialize accumulators for weighted average calculations\n",
    "total_train_score = 0\n",
    "total_val_score = 0\n",
    "total_test_score = 0\n",
    "total_samples_train = 0\n",
    "total_samples_val = 0\n",
    "total_samples_test = 0\n",
    "\n",
    "for zone, station in zone_station_map.items():\n",
    "    # Filter data for this zone and station\n",
    "    zone_data = combined_data[(combined_data['zone_id'] == zone) & (combined_data['station_id'] == station)]\n",
    "\n",
    "    # Apply specific filter for zone 15\n",
    "    if zone == 15:\n",
    "        zone_data = zone_data[zone_data['load'] < 200000]  # Exclude load values above 200,000 for zone 15\n",
    "    \n",
    "    # Features and target\n",
    "    X = zone_data[['temperature', 'hour']]\n",
    "    y = zone_data['load']\n",
    "    \n",
    "    # Split data into training/validation (80%) and test (20%) sets\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Further split the training/validation data into training (80%) and validation (20%) sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "    \n",
    "    # Set up GridSearchCV to find the best parameters\n",
    "    grid_search = GridSearchCV(GradientBoostingRegressor(random_state=42), param_grid, cv=3, scoring='r2')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Retrieve the best estimator\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Calculate scores\n",
    "    train_score = best_model.score(X_train, y_train)\n",
    "    val_score = best_model.score(X_val, y_val)\n",
    "    test_score = best_model.score(X_test, y_test)\n",
    "    \n",
    "    # Accumulate scores weighted by number of samples\n",
    "    total_train_score += train_score * len(y_train)\n",
    "    total_val_score += val_score * len(y_val)\n",
    "    total_test_score += test_score * len(y_test)\n",
    "    total_samples_train += len(y_train)\n",
    "    total_samples_val += len(y_val)\n",
    "    total_samples_test += len(y_test)\n",
    "\n",
    "    # Output best parameters and scores for each zone\n",
    "    print(f'Zone {zone}: Best Parameters = {grid_search.best_params_}')\n",
    "    print(f'Zone {zone}: Training Score = {train_score:.4f}, Validation Score = {val_score:.4f}, Test Score = {test_score:.4f}')\n",
    "\n",
    "# Calculate weighted average scores\n",
    "general_train_score = total_train_score / total_samples_train\n",
    "general_val_score = total_val_score / total_samples_val\n",
    "general_test_score = total_test_score / total_samples_test\n",
    "\n",
    "# Output the general scores\n",
    "print(f'General Training Score: {general_train_score:.4f}')\n",
    "print(f'General Validation Score: {general_val_score:.4f}')\n",
    "print(f'General Test Score: {general_test_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "265e9318-f28f-4715-aad7-3b1edb4f2efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Training Score: 0.7178\n",
      "General Validation Score: 0.7047\n",
      "General Test Score: 0.7095\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load the dataset\n",
    "combined_data = pd.read_csv('combined_data.csv')\n",
    "combined_data['datetime'] = pd.to_datetime(combined_data['datetime'])\n",
    "combined_data['hour'] = pd.to_numeric(combined_data['hour'].str.extract('(\\d+)')[0])\n",
    "\n",
    "# Zone to station mapping based on best correlation\n",
    "zone_station_map = {\n",
    "    1: 8, 2: 10, 3: 1, 4: 8, 5: 8, 6: 1, 7: 1, 8: 6, \n",
    "    9: 1, 10: 8, 11: 1, 12: 10, 13: 8, 14: 1, 15: 3, \n",
    "    16: 10, 17: 1, 18: 1, 19: 2, 20: 8\n",
    "}\n",
    "\n",
    "# Initialize accumulators for weighted average calculations\n",
    "total_train_score = 0\n",
    "total_val_score = 0\n",
    "total_test_score = 0\n",
    "total_samples_train = 0\n",
    "total_samples_val = 0\n",
    "total_samples_test = 0\n",
    "\n",
    "for zone, station in zone_station_map.items():\n",
    "    # Filter data for this zone and station\n",
    "    zone_data = combined_data[(combined_data['zone_id'] == zone) & (combined_data['station_id'] == station)]\n",
    "\n",
    "    # Apply specific filter for zone 15\n",
    "    if zone == 15:\n",
    "        zone_data = zone_data[zone_data['load'] < 200000]  # Exclude load values above 200,000 for zone 15\n",
    "    \n",
    "    # Features and target\n",
    "    X = zone_data[['temperature', 'hour']]\n",
    "    y = zone_data['load']\n",
    "    \n",
    "    # Split data into training/validation (80%) and test (20%) sets\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Further split the training/validation data into training (80%) and validation (20%) sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "    \n",
    "    # Initialize and train the Gradient Boosting Regressor\n",
    "    model = GradientBoostingRegressor(learning_rate = 0.2, max_depth = 3, n_estimators = 150, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate scores\n",
    "    train_score = model.score(X_train, y_train)\n",
    "    val_score = model.score(X_val, y_val)\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    \n",
    "    # Accumulate scores weighted by number of samples\n",
    "    total_train_score += train_score * len(y_train)\n",
    "    total_val_score += val_score * len(y_val)\n",
    "    total_test_score += test_score * len(y_test)\n",
    "    total_samples_train += len(y_train)\n",
    "    total_samples_val += len(y_val)\n",
    "    total_samples_test += len(y_test)\n",
    "\n",
    "# Calculate weighted average scores\n",
    "general_train_score = total_train_score / total_samples_train\n",
    "general_val_score = total_val_score / total_samples_val\n",
    "general_test_score = total_test_score / total_samples_test\n",
    "\n",
    "# Output the general scores\n",
    "print(f'General Training Score: {general_train_score:.4f}')\n",
    "print(f'General Validation Score: {general_val_score:.4f}')\n",
    "print(f'General Test Score: {general_test_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "272e6a3b-e799-4e11-86c3-116d92062125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "                       \n",
    "# Load the dataset\n",
    "combined_data = pd.read_csv('combined_data.csv')\n",
    "combined_data['datetime'] = pd.to_datetime(combined_data['datetime'])\n",
    "combined_data['hour'] = pd.to_numeric(combined_data['hour'].str.extract('(\\d+)')[0])\n",
    "\n",
    "# Simulate loading temperature data\n",
    "temp_data = pd.read_csv('../Temp_history_final.csv')\n",
    "\n",
    "# Prepare the DataFrame to store all predictions\n",
    "all_predictions = []\n",
    "\n",
    "# Zone to station mapping based on best correlation\n",
    "zone_station_map = {\n",
    "    1: 8, 2: 10, 3: 1, 4: 8, 5: 8, 6: 1, 7: 1, 8: 6, \n",
    "    9: 1, 10: 8, 11: 1, 12: 10, 13: 8, 14: 1, 15: 3, \n",
    "    16: 10, 17: 1, 18: 1, 19: 2, 20: 8\n",
    "}\n",
    "\n",
    "# Train a model for each zone using its specific station's historical data\n",
    "for zone, station in zone_station_map.items():\n",
    "    # Filter data for this zone and station\n",
    "    zone_data = combined_data[(combined_data['zone_id'] == zone) & (combined_data['station_id'] == station)]\n",
    "    X = zone_data[['temperature', 'hour']]\n",
    "    y = zone_data['load']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = GradientBoostingRegressor(learning_rate=0.2, max_depth=3, n_estimators=150, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict for specific days using temperature forecasts\n",
    "    for day in range(1, 8):  # June 1 to June 7\n",
    "        for hour in range(1, 25):  # 24 hours, adjusting index for correct hour labels\n",
    "            hour_col = 'h' + str(hour)\n",
    "            if hour_col in temp_data.columns and not temp_data[temp_data['station_id'] == station][hour_col].isnull().all():\n",
    "                hourly_temp = temp_data.loc[(temp_data['station_id'] == station) & (temp_data['day'] == day), hour_col].values\n",
    "                if hourly_temp.size > 0:\n",
    "                    # Prepare the feature array for prediction\n",
    "                    features = np.array([[hourly_temp[0], hour - 1]])  # Adjust hour for zero-indexing in the model\n",
    "                    prediction = model.predict(features.reshape(1, -2))\n",
    "                    # Store the prediction\n",
    "                    all_predictions.append({\n",
    "                        'zone_id': zone,\n",
    "                        'year': 2008,\n",
    "                        'month': 6,\n",
    "                        'day': day,\n",
    "                        'hour': hour,  # Use the correct hour for output\n",
    "                        'predicted_load': prediction[0]\n",
    "                    })\n",
    "\n",
    "# Convert predictions list to DataFrame\n",
    "predictions_df = pd.DataFrame(all_predictions)\n",
    "\n",
    "# Pivot DataFrame to wide format to match required output\n",
    "pivot_df = predictions_df.pivot_table(index=['zone_id', 'year', 'month', 'day'], columns='hour', values='predicted_load', aggfunc='first').reset_index()\n",
    "pivot_df.columns = [f'h{col}' if isinstance(col, int) else col for col in pivot_df.columns]\n",
    "\n",
    "# Save to CSV\n",
    "pivot_df.to_csv('Load_prediction.csv', index=False)\n",
    "\n",
    "warnings.filterwarnings(action='default', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48fa15f-314b-49ab-b81a-0875dce0bb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
